

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>ETL principles &mdash; ETL Best Practices with Airflow v1.8</title>
  

  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  

  

  
        <link rel="index" title="Index"
              href="genindex.html"/>
        <link rel="search" title="Search" href="search.html"/>
    <link rel="top" title="ETL Best Practices with Airflow v1.8" href="index.html"/>
        <link rel="next" title="Gotcha’s" href="gotchas.html"/>
        <link rel="prev" title="ETL best practices with Airflow documentation site" href="index.html"/> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html" class="icon icon-home"> ETL Best Practices with airflow 1.8
          

          
          </a>

          
            
            
              <div class="version">
                1.8
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
                <ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">ETL principles</a></li>
<li class="toctree-l1"><a class="reference internal" href="gotchas.html">Gotcha&#8217;s</a></li>
<li class="toctree-l1"><a class="reference internal" href="great.html">What makes Airflow great?</a></li>
<li class="toctree-l1"><a class="reference internal" href="etlexample.html">ETL example</a></li>
<li class="toctree-l1"><a class="reference internal" href="hiveexample.html">Hive example</a></li>
<li class="toctree-l1"><a class="reference internal" href="monitoring.html">Monitoring</a></li>
<li class="toctree-l1"><a class="reference internal" href="platform.html">Building your own ETL platform</a></li>
<li class="toctree-l1"><a class="reference internal" href="ingestfile.html">Ingesting files</a></li>
<li class="toctree-l1"><a class="reference internal" href="tips.html">Tips</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="index.html">ETL Best Practices with airflow 1.8</a>
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          

 



<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="index.html">Docs</a> &raquo;</li>
      
    <li>ETL principles</li>
      <li class="wy-breadcrumbs-aside">
        
          
        
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="etl-principles">
<h1>ETL principles<a class="headerlink" href="#etl-principles" title="Permalink to this headline">¶</a></h1>
<p>Before we start diving into airflow and solving problems using specific tools,
let&#8217;s collect and analyze important ETL best practices and gain a better understanding
of those principles, why they are needed and what they solve for you in the long run.</p>
<p>The best practices make a lot more sense when you work for a fairly large organization that
has a large number of data sources, calculation processes, maybe a bit of data science and
maybe even big data. The best practices will also remind you how ad-hoc scheduling and
&#8216;solving it quick to get going&#8217; approaches eventually create a tangled mess of data problems,
inconsistencies and downstream issues that are difficult to analyze and solve.</p>
<p>The nice thing about airflow is that it is designed around the following best practices
already, so implementing your new workflows is relatively easy. The reason that I started this
site is because I want to help people get better results with their new setup by indicating
usage patterns in DAGs and how to solve specific problems.</p>
<p>Airflow follows a nice, meditated philosophy on how ETL jobs should be structured. This philosophy
enables airflow to parallelize jobs, schedule them appropriately with dependencies and historically
reprocess data when needed. This philosophy is rooted in a couple of simple principles:</p>
<p><strong>Load data incrementally</strong>:  When a table or dataset is small, you can afford to extract it as a whole and
write it to the destination. As an organization grows however, you’ll need to extract data incrementally
at regular intervals and only load data for an hour, day, week, etc. Airflow makes it very easy to schedule
jobs such that they process specific intervals with job parameters that allow you select data.</p>
<p><strong>Process historic data</strong>:  There are cases when you just finished a new workflow and need data that
goes back further than the date push your new code into production. In this situation you can simply use the
<em>start_date</em> parameter in a DAG to specify the start date. Airflow will then back-fill tasks to process that data
all the way back to that start date. It may not be appropriate or desirable to have so many execution runs to get
data up-to-date, so there are some other strategies that you can use to process weeks, months or years of data through
better parametrization of the DAGS. That will be documented in a separate story on this site. I&#8217;ve personally seen
cases where a lot of effort went into dealing with historical data loads and a lot of manual coding and workarounds
to achieve this. The idea is to make this effort repeatable and simple.</p>
<p><strong>Partition ingested data</strong>: By partitioning data being ingested at the destination, you can parallellize dag runs,
avoid write locks on data being ingested and optimize performance when that same data is being read. It will also
serve as a historical snapshot of what the data looked like at specific moments in time for audit purposes. Partitions
that are no longer relevant can be archived and removed from the database.</p>
<p><strong>Enforce the idempotency constraint</strong>:  The result of a DAG run should always have idempotency characteristics. This means that when you
run a process multiple times with the same parameters (even on different days), the outcome is exactly the same. You do
not end up with multiple copies of the same data in your environment or other undesirable side effects. This is obviously
only valid when the processing itself has not been modified. If business rules change within the process, then the target
data will be different. It&#8217;s a good idea here to be aware of auditors or other business requirements on reprocessing historic
data, because it&#8217;s not always allowed. Also, some processes require anonimization of data after a certain number of days,
because it&#8217;s not always allowed to keep historical customer data on record <em>forever</em>.</p>
<p><strong>Execute conditionally</strong>:  Airflow has some options to control how tasks within DAGs are run based on the success of the
instance that came before it. For example, the <em>depends_on_past</em> parameter specifies that all task instances before the
one being executed must have succeeded before it executes the current one. The recently introduced <em>LatestOnlyOperator</em>
allows you to conditionally skip tasks downstream in the DAG if it’s not the most recent execution. There’s also the
<em>BranchPythonOperator</em>, which can select which branch of execution proceeds in the DAG based on some decision function.</p>
<p><strong>Code workflow as well as the application</strong>:  Most schedulers and workflow tools allow you to create steps and then define either in the UI
or through XML how the steps are wired together. Eventually there will always be that particular job that becomes limited,
because the logic of the workflow is not compatible with what you can do in the tool. In airflow, you do not just code
the application process, you also code the workflow process itself. You can start new DAG&#8217;s dynamically from within a DAG,
skip downstream tasks programmatically, use python functions to conditionally execute other code, run sub-dags and so on.</p>
<p><strong>Rest data between tasks</strong>: To allow airflow to run on multiple workers and even parallelize task instances within
the same DAG, you need to think where you save data in between steps. An airflow operator would typically read from one system,
create a temporary local file, then write that file to some destination system. What you should not and even cannot do is depend on
temporary data (files, etc.) that is created by one task in other tasks downstream. The reason is that task instances of the
same DAG can get executed on different workers and that local resource won’t be there. When you use the Sequential or LocalExecutor
you may seem to be able to get away with it, but if there is a need to use a distributed method of execution in the future
and you have these dependencies, it can be very painful to fix them for each dag. It is therefore a good idea to ensure that
data is read from services that are accessible to all workers and that you have data at rest within those services when
tasks start and terminate.</p>
<p><strong>Understand SLA’s and alerts</strong>: Service Level Agreements can be used to detect long running task instances and let airflow take further action.
By default, airflow will email the missed SLA’s and these can be inspected further from the web server. In the DAG, you can
specify a function that is to be called when there is an SLA miss, which is how you can communicate this to other communication channels.</p>
<p><strong>Parameterize subflows and dynamically run tasks</strong>: Because the workflow is code, it is possible to dynamically create tasks or even
complete dags through that code. There are some examples where people have a text file with instructions what they want processed and
airflow simply uses that file to dynamically generate a dag with some parameterized tasks specific to that instruction file.
There are downsides to this, for example if you generate new dag_id’s when the DAG is created, the web server will not show
historical executions anymore, because it thinks the dag has been removed.</p>
<p>The use of dynamic workflow specifications becomes more interesting within the context of subflows. If you have some routine code,
for example checking the number of rows in a database and sending that as a metric to some service or potentially do some other checks,
you can design that work as a “subdag” and use a factory method in a library to instantiate this functionality.</p>
<p><strong>Pool your resources</strong>: Simple schedulers keep no control over the use of resources within scripts. In airflow you can actually
create resource pools and require tasks to acquire a token from this pool before doing any work. If the pool is fully used up,
other tasks that require the token will not be scheduled until another token becomes available when another task finishes.
This is very useful if you want to manage access to shared resources like a database, a gpu, a cpu, etc.</p>
<p><strong>Sense when to start a task</strong>: A lot of schedulers start tasks when a specific date / time has passed, exactly what cron does.
Airflow can do this simple behavior, but it can also sense the conditions that are required before the rest of the dag will
continue by using a sensor as the first task in the dag. Example sensors include a dag dependency sensor
(which is triggered by a task instance result in another dag), an HTTP sensor that calls a URL and parses the result and sensors
that detect the presence of files on a file system or data in a database. This provides a lot of tools to guarantee consistency
in the overall ETL pipeline.</p>
<p><strong>Manage login details in one place</strong>: Airflow maintains the login details for external services in its own database. You can refer to those
configurations simply by referring to the name of that connection and airflow makes it available to the operator, sensor or hook.
Not all schedulers do this properly; sometimes the workflow files contain these details (leading to duplication everywhere
and a pain to update when it changes).</p>
<p><strong>Specify configuration details once</strong>: Following the DRY principle, avoid duplication of configuration details by specifying them in a single
place once and look up the correct configuration from the code. Each airflow instance has &#8220;Variables&#8221; that can be set and looked up to
specify the context in which a task is run.</p>
<p><strong>Keep all metadata in one place</strong>: You don’t need to do anything here. Airflow will manage logs, job duration, landing times in one place,
which reduces the amount of overhead on people to collect this metadata in order to analyze problems.</p>
<p><strong>Develop your own workflow framework</strong>: Code as a workflow also allows you to reuse parts of DAG’s if you need to, reducing code duplication
and making things simpler in the long run. This reduces the complexity of the overall system and frees up developer time to work on more
important and impactful tasks.</p>
</div>


           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="gotchas.html" class="btn btn-neutral float-right" title="Gotcha’s" accesskey="n">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="index.html" class="btn btn-neutral" title="ETL best practices with Airflow documentation site" accesskey="p"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2016, Gerard Toonstra.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'1.8',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>

  

  
  
    <script type="text/javascript" src="_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>